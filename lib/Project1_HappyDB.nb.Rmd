---
title: "The Happy Moments of Males"
output: html_notebook

---

```{r load libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
```


```{r read data, warning=FALSE, message=FALSE, echo=FALSE}
hm_data <- read_csv("../data/cleaned_hm.csv")
```

```{r text processing in tm, warning=FALSE, message=FALSE, echo=FALSE}
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace)
```

```{r stemming, warning=FALSE, message=FALSE, echo=FALSE}
stemmed <- tm_map(corpus, stemDocument) %>%
  tidy() %>% #coverting "tm" to "tidy" for faster processing
  select(text)
```

```{r tidy dictionary, warning=FALSE, message=FALSE, echo=FALSE}
dict <- tidy(corpus) %>% #coverting to tidy format
  select(text) %>%
  unnest_tokens(dictionary, text)
```

```{r stopwords, warning=FALSE, message=FALSE, echo=FALSE}
data("stop_words")

word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past")

stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))
```

```{r tidy stems with dictionary, warning=FALSE, message=FALSE, echo=FALSE}
completed <- stemmed %>%
  mutate(id = row_number()) %>%
  unnest_tokens(stems, text) %>%
  bind_cols(dict) %>%
  anti_join(stop_words, by = c("dictionary" = "word"))  #combine the stems and the dictionary into the same "tidy" object
```

```{r stem completion, warning=FALSE, message=FALSE, echo=FALSE}
completed <- completed %>%
  group_by(stems) %>%
  count(dictionary) %>%
  mutate(word = dictionary[which.max(n)]) %>%
  ungroup() %>%
  select(stems, word) %>%
  distinct() %>%
  right_join(completed) %>%
  select(-stems)
```

```{r reverse unnest, warning=FALSE, message=FALSE, echo=FALSE}
completed <- completed %>%
  group_by(id) %>%
  summarise(text = str_c(word, collapse = " ")) %>%
  ungroup()
```

```{r cleaned hm_data, warning=FALSE, message=FALSE, echo=FALSE}
hm_data <- hm_data %>%
  mutate(id = row_number()) %>%
  inner_join(completed)

#datatable(hm_data)
```


```{r export data, warning=FALSE, message=FALSE, echo=FALSE}
write_csv(hm_data, "../output/processed_moments.csv")
```


```{r ,message=FALSE, warning=FALSE,echo=FALSE}
#Read files and combine files
df<- read_csv("../output/processed_moments.csv")
demo_data <- read_csv("../data/demographic.csv")

df <- df %>%
  inner_join(demo_data, by = "wid")  %>%
  select(c("text","country","gender","marital","cleaned_hm","parenthood"))

#filter out the male data
df_male <-df %>%
  filter(gender %in% c('m') )


#create stopwords
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past","finally","found","favorite","received","happiness","excited","surprise","beautiful","fun","delicious","celebrated","pay","expected","prepared","share"

)

my_stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))


```

\
\


###What are those sentiments from males?

```{r   , message=FALSE,  warning=FALSE,echo=FALSE}
# male's sentiment 
df_male %>%
  unnest_tokens(word,text)%>%
  anti_join(my_stop_words) %>%
  inner_join(get_sentiments("nrc"))%>%
  group_by(sentiment)%>% count(word)%>% summarize(Score = sum(n))%>%arrange(desc(Score))%>%
  ggplot(aes(x=reorder(sentiment,-Score), y = Score))+
  geom_bar(aes(fill=sentiment),stat = "identity")+
  xlab("sentiment") + ylab("Score") + ggtitle("sentiment of males")


```

Looking at the sentiment from male's happy moments, we can find that positive, joy,trust,and anticipation contribute most of the sentiments. For the rest of the sentiments such as negative, sadness , fear , anger ,and disgust, they don't play a big part in male's happy moments.So what are those words contributing to the joy sentiment?



###The most common joy words from males.
```{r,message=FALSE, warning=FALSE, echo=FALSE}

# explore male's joy sentiment
  df_male %>%
  unnest_tokens(word,text)%>%
  anti_join(my_stop_words) %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(sentiment %in% c("joy"))%>%
  count(word, sentiment, sort = TRUE) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ####plot most common joy words of males
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE,fill = "springgreen3") +
  labs(y = "Contribution to sentiment",
       x = NULL)+coord_flip()

```

From the plot, We can see that friend is the most frequent word. Birthday is the second. Also, Males really enjoy talking about thier family especially their daughter and mother. Of course, males are happy with money,food,shopping and vacation.
\
\

 
##Single vs Married
\



###Wordcloud 

```{r, message=FALSE, warning=FALSE,fig.height=6, fig.width=6,echo=FALSE}
#split males into single and married
male_single <- df_male%>%filter(marital =="single")
male_married<- df_male%>%filter(marital =="married")

#wordcloud
corpus<-Corpus(VectorSource(male_single$text))
corpus <- tm_map(corpus, stripWhitespace)
tdm.all<-TermDocumentMatrix(corpus)
tdm.tidy=tidy(tdm.all)
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))

corpus1<-Corpus(VectorSource(male_married$text))
corpus1 <- tm_map(corpus1, stripWhitespace)
tdm.all1<-TermDocumentMatrix(corpus1)
tdm.tidy1=tidy(tdm.all1)
tdm.overall1=summarise(group_by(tdm.tidy1, term), sum(count))


par(mfrow=c(2,1), las=1, mar=c(3.1, 4.1, 1.1, 2.1))
###male married word cloud

wordcloud(tdm.overall1$term, tdm.overall1$`sum(count)`,
          scale=c(3,0.5),
          max.words=20,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(7,"Greens"))

###male single word cloud
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(3,0.5),
          max.words=20,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))


```

Comparing with the two word clouds,the top one is from married males and it's clear that married males talk about thier wife most of time and they focus more on thier family members such as son and daughter. The bottom is from single males and we find that they care more about their friends most of time. Besides, single males talk more about girlfriend.

\
\


### Big difference in joy sentiment?

```{r ,  message=FALSE, warning=FALSE,echo=FALSE}
word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past","finally","found","favorite","received","happiness","excited","surprise","beautiful","fun","delicious","celebrated","pay","expected","prepared","share","friend","money","unexpected","love","food","green","birthday","score","mother","shopping","smile","gift","pretty","vacation","laugh","special","joy","perfect","wonderful","pleasant","bonus","graduation"
)

sm_stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))



require(gridExtra) # For combining two plots
####married male joy
plot1<- male_married%>%
  unnest_tokens(word,text)%>%
  anti_join(sm_stop_words) %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(sentiment %in% c("joy"))%>%
  count(word, sentiment, sort = TRUE) %>%
  top_n(5) %>%
  mutate(word = reorder(word, n)) %>%
  ####plot most common joy words of married males
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE,fill = "lightblue") +
  labs(y = "Contribution to sentiment",
       x = NULL, title="Married")+ 
  coord_flip()

####single male joy
plot2 <- male_single%>%
  unnest_tokens(word,text)%>%
  anti_join(sm_stop_words) %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(sentiment %in% c("joy"))%>%
  count(word, sentiment, sort = TRUE) %>%
  top_n(5) %>%
  mutate(word = reorder(word, n)) %>%
    ####plot most common joy words of single males
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE,fill = "lightgoldenrod1") +
  labs(y = "Contribution to sentiment",
       x = NULL, title="Single")+
  coord_flip()

grid.arrange(plot1, plot2, ncol=2)

```

In order to have a clear comparison, I removed the same common words such as "friend","money","love","food" and "birthday". Then we can find that married males have happy moments when they spend time with their family and they care more about thier children and marriage. On the other hand, the single males enjoy music, cream, and beer. 




\
\


###Which country contributes most of words?
```{r message=FALSE, warning=FALSE,echo=FALSE}
# countries contributing most words
 df_male%>%unnest_tokens(word,cleaned_hm)%>%group_by(country)%>%count(word)%>%summarise(total_words = sum(n))%>%arrange(desc(total_words))%>%top_n(5)%>%ungroup()%>%
  ggplot(aes(x=reorder(country,total_words) , y = total_words))+
  geom_bar(aes(fill=country),stat = "identity")+
  xlab("country") + ylab("words") + ggtitle("words contributed from countries")

```

As shown above, most of male words are from America and India. So we can think that the sentiment mainly focus on American and Indian males.


\
\


###What about male's joy sentiment in other countries ?

```{r message=FALSE, warning=FALSE,echo=FALSE}
#remove extra stopwords
word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past","finally","found","favorite","received","happiness","excited","surprise","beautiful","fun","delicious","celebrated","pay","expected","unexpected","prepared","share","friend","unexpected","love","green","birthday","score","mother","wonderful","smile","gift","pretty","outstanding","money","daughter")



new_stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))


###USA anticipation sentient
require(gridExtra) # For combining two plots

###VEN anticipation sentient

p1<-df_male%>%
  filter( country == "VEN")%>%
  unnest_tokens(word,text)%>%
  anti_join(new_stop_words) %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(sentiment %in% c("joy"))%>%
  count(word, sentiment, sort = TRUE) %>%
  top_n(5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE,fill = "red") +
  labs(y = "Contribution to sentiment",
       x = NULL, title="Venezuela")+
  coord_flip()

###GBR anticipation sentient

p2<-df_male%>%
  filter( country == "GBR")%>%
  unnest_tokens(word,text)%>%
  anti_join(new_stop_words) %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(sentiment %in% c("joy"))%>%
  count(word, sentiment, sort = TRUE) %>%
  top_n(5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE,fill = "royalblue") +
  labs(y = "Contribution to sentiment",
       x = NULL, title="United Kingdom")+
  coord_flip()


grid.arrange(p1, p2,ncol=2)

```

From the plot, it's interesting that males of Venezuela are happy about food especially they love chocolate and they also love dancing. While British males really love association football and they enjoy music as well.





###Topic modeling

```{r , message=FALSE,  warning=FALSE,echo=FALSE}


corpus.list=df_male[2:(nrow(df_male)-1), ]
sentence.pre=df_male$cleaned_hm[1:(nrow(df_male)-2)]
sentence.post=df_male$cleaned_hm[3:(nrow(df_male)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$cleaned_hm, sentence.post, sep=" ")



docs <- Corpus(VectorSource(corpus.list$snipets))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
#remove punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
#Strip digits
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
#Stem document
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

dtm <- DocumentTermMatrix(docs)

#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 9

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))


#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv"))

#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv"))

ldaOut_topics <- tidy(ldaOut, matrix = "beta")

top_terms <- ldaOut_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")

```

From the nine topics above, I cluster them into the following topics.

Topic 2 : made great dinner wife night  

Topic 3 : enjoy time family home  

Topic 4 : took hour walk dog 

Topic 5 : get final project work complet

Topic 6 : play game watch movie video

Topic 7 : birthday daughter son love 

Topic 1,8,9 are hard to tell 



\
\


###Reference
Silge,Robinson,2018-12-21,"Text Mining with R". https://www.tidytextmining.com/index.html
https://github.com/TZstatsADS/ADS_Teaching/blob/master/Tutorials/wk2-TextMining/doc/InteractiveWordCloud.Rmd
https://github.com/TZstatsADS/ADS_Teaching/blob/master/Tutorials/wk2-TextMining/doc/wk2-Tutorial-TextMining.Rmd
https://github.com/TZstatsADS/ADS_Teaching/blob/master/Projects_StarterCodes/Project1-RNotebook/doc/Text_Processing.Rmd
